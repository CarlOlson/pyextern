/* This file is generated, do not edit! */
package pandas.io.data;
@:pythonImport("pandas.io.data") extern class Data_Module {
	static public var CUR_DAY : Dynamic;
	static public var CUR_MONTH : Dynamic;
	static public var CUR_YEAR : Dynamic;
	/**
		Imports data from a number of online sources.
		
		Currently supports Yahoo! Finance, Google Finance, St. Louis FED (FRED)
		and Kenneth French's data library.
		
		Parameters
		----------
		name : str or list of strs
		    the name of the dataset. Some data sources (yahoo, google, fred) will
		    accept a list of names.
		data_source: str, default: None
		    the data source ("yahoo", "google", "fred", or "ff")
		start : datetime, default: None
		    left boundary for range (defaults to 1/1/2010)
		end : datetime, default: None
		    right boundary for range (defaults to today)
		retry_count : int, default 3
		    Number of times to retry query request.
		pause : numeric, default 0.001
		    Time, in seconds, to pause between consecutive queries of chunks. If
		    single value given for symbol, represents the pause between retries.
		
		Examples
		----------
		
		# Data from Yahoo! Finance
		gs = DataReader("GS", "yahoo")
		
		# Data from Google Finance
		aapl = DataReader("AAPL", "google")
		
		# Data from FRED
		vix = DataReader("VIXCLS", "fred")
		
		# Data from Fama/French
		ff = DataReader("F-F_Research_Data_Factors", "famafrench")
		ff = DataReader("F-F_Research_Data_Factors_weekly", "famafrench")
		ff = DataReader("6_Portfolios_2x3", "famafrench")
		ff = DataReader("F-F_ST_Reversal_Factor", "famafrench")
	**/
	static public function DataReader(name:Dynamic, ?data_source:Dynamic, ?start:Dynamic, ?end:Dynamic, ?retry_count:Dynamic, ?pause:Dynamic):Dynamic;
	static public function ZipFile(args:Dynamic, kwargs:Dynamic):Dynamic;
	static public var _FAMAFRENCH_URL : Dynamic;
	static public var _FRED_URL : Dynamic;
	static public var _HISTORICAL_GOOGLE_URL : Dynamic;
	static public var _HISTORICAL_YAHOO_URL : Dynamic;
	static public var _YAHOO_COMPONENTS_URL : Dynamic;
	static public var _YAHOO_QUOTE_URL : Dynamic;
	static public var __builtins__ : Dynamic;
	static public var __cached__ : Dynamic;
	static public var __doc__ : Dynamic;
	static public var __file__ : Dynamic;
	static public var __loader__ : Dynamic;
	static public var __name__ : Dynamic;
	static public var __package__ : Dynamic;
	static public var __spec__ : Dynamic;
	static public var __warningregistry__ : Dynamic;
	/**
		Return modifed DataFrame or Panel with adjusted prices based on
		'Adj Close' price. Adds 'Adj_Ratio' column.
	**/
	static public function _adjust_prices(hist_data:Dynamic, ?price_list:Dynamic):Dynamic;
	/**
		Return a returns index from a input price df or series. Initial value
		(typically NaN) is set to 1.
	**/
	static public function _calc_return_index(price_df:Dynamic):Dynamic;
	static public function _dl_mult_symbols(symbols:Dynamic, start:Dynamic, end:Dynamic, interval:Dynamic, chunksize:Dynamic, retry_count:Dynamic, pause:Dynamic, method:Dynamic):Dynamic;
	static public function _get_data_from(symbols:Dynamic, start:Dynamic, end:Dynamic, interval:Dynamic, retry_count:Dynamic, pause:Dynamic, adjust_price:Dynamic, ret_index:Dynamic, chunksize:Dynamic, source:Dynamic):Dynamic;
	/**
		Get historical data for the given name from google.
		Date format is datetime
		
		Returns a DataFrame.
	**/
	static public function _get_hist_google(sym:Dynamic, start:Dynamic, end:Dynamic, interval:Dynamic, retry_count:Dynamic, pause:Dynamic):Dynamic;
	/**
		Get historical data for the given name from yahoo.
		Date format is datetime
		
		Returns a DataFrame.
	**/
	static public function _get_hist_yahoo(sym:Dynamic, start:Dynamic, end:Dynamic, interval:Dynamic, retry_count:Dynamic, pause:Dynamic):Dynamic;
	/**
		Return sequence in 'chunks' of size defined by size
	**/
	static public function _in_chunks(seq:Dynamic, size:Dynamic):Dynamic;
	static public var _network_error_classes : Dynamic;
	static public function _retry_read_url(url:Dynamic, retry_count:Dynamic, pause:Dynamic, name:Dynamic):Dynamic;
	static public function _sanitize_dates(start:Dynamic, end:Dynamic):Dynamic;
	static public var _source_functions : Dynamic;
	static public function _two_char(s:Dynamic):Dynamic;
	static public var _yahoo_codes : Dynamic;
	static public function bytes_to_str(b:Dynamic, ?encoding:Dynamic):Dynamic;
	/**
		Concatenate pandas objects along a particular axis with optional set logic
		along the other axes. Can also add a layer of hierarchical indexing on the
		concatenation axis, which may be useful if the labels are the same (or
		overlapping) on the passed axis number
		
		Parameters
		----------
		objs : a sequence or mapping of Series, DataFrame, or Panel objects
		    If a dict is passed, the sorted keys will be used as the `keys`
		    argument, unless it is passed, in which case the values will be
		    selected (see below). Any None objects will be dropped silently unless
		    they are all None in which case a ValueError will be raised
		axis : {0, 1, ...}, default 0
		    The axis to concatenate along
		join : {'inner', 'outer'}, default 'outer'
		    How to handle indexes on other axis(es)
		join_axes : list of Index objects
		    Specific indexes to use for the other n - 1 axes instead of performing
		    inner/outer set logic
		verify_integrity : boolean, default False
		    Check whether the new concatenated axis contains duplicates. This can
		    be very expensive relative to the actual data concatenation
		keys : sequence, default None
		    If multiple levels passed, should contain tuples. Construct
		    hierarchical index using the passed keys as the outermost level
		levels : list of sequences, default None
		    Specific levels (unique values) to use for constructing a
		    MultiIndex. Otherwise they will be inferred from the keys
		names : list, default None
		    Names for the levels in the resulting hierarchical index
		ignore_index : boolean, default False
		    If True, do not use the index values along the concatenation axis. The
		    resulting axis will be labeled 0, ..., n - 1. This is useful if you are
		    concatenating objects where the concatenation axis does not have
		    meaningful indexing information. Note the the index values on the other
		    axes are still respected in the join.
		copy : boolean, default True
		    If False, do not copy data unnecessarily
		
		Notes
		-----
		The keys, levels, and names arguments are all optional
		
		Returns
		-------
		concatenated : type of objects
	**/
	static public function concat(objs:Dynamic, ?axis:Dynamic, ?join:Dynamic, ?join_axes:Dynamic, ?ignore_index:Dynamic, ?keys:Dynamic, ?levels:Dynamic, ?names:Dynamic, ?verify_integrity:Dynamic, ?copy:Dynamic):Dynamic;
	/**
		Returns DataFrame containing list of component information for
		index represented in idx_sym from yahoo. Includes component symbol
		(ticker), exchange, and name.
		
		Parameters
		----------
		idx_sym : str
		    Stock index symbol
		    Examples:
		    '^DJI' (Dow Jones Industrial Average)
		    '^NYA' (NYSE Composite)
		    '^IXIC' (NASDAQ Composite)
		
		    See: http://finance.yahoo.com/indices for other index symbols
		
		Returns
		-------
		idx_df : DataFrame
	**/
	static public function get_components_yahoo(idx_sym:Dynamic):Dynamic;
	static public function get_data_famafrench(name:Dynamic):Dynamic;
	/**
		Get data for the given name from the St. Louis FED (FRED).
		Date format is datetime
		
		Returns a DataFrame.
		
		If multiple names are passed for "series" then the index of the
		DataFrame is the outer join of the indicies of each series.
	**/
	static public function get_data_fred(name:Dynamic, ?start:Dynamic, ?end:Dynamic):Dynamic;
	/**
		Returns DataFrame/Panel of historical stock prices from symbols, over date
		range, start to end. To avoid being penalized by Google Finance servers,
		pauses between downloading 'chunks' of symbols can be specified.
		
		Parameters
		----------
		symbols : string, array-like object (list, tuple, Series), or DataFrame
		    Single stock symbol (ticker), array-like object of symbols or
		    DataFrame with index containing stock symbols.
		start : string, (defaults to '1/1/2010')
		    Starting date, timestamp. Parses many different kind of date
		    representations (e.g., 'JAN-01-2010', '1/1/10', 'Jan, 1, 1980')
		end : string, (defaults to today)
		    Ending date, timestamp. Same format as starting date.
		retry_count : int, default: 3
		    Number of times to retry query request.
		pause : numeric, default: 0.001
		    Time, in seconds, to pause between consecutive queries of chunks. If
		    single value given for symbol, represents the pause between retries.
		chunksize : int, default: 25
		    Number of symbols to download consecutively before intiating pause.
		ret_index : bool, default: False
		    If True, includes a simple return index 'Ret_Index' in hist_data.
		
		Returns
		-------
		hist_data : DataFrame (str) or Panel (array-like object, DataFrame)
	**/
	static public function get_data_google(?symbols:Dynamic, ?start:Dynamic, ?end:Dynamic, ?retry_count:Dynamic, ?pause:Dynamic, ?adjust_price:Dynamic, ?ret_index:Dynamic, ?chunksize:Dynamic):Dynamic;
	/**
		Returns DataFrame/Panel of historical stock prices from symbols, over date
		range, start to end. To avoid being penalized by Yahoo! Finance servers,
		pauses between downloading 'chunks' of symbols can be specified.
		
		Parameters
		----------
		symbols : string, array-like object (list, tuple, Series), or DataFrame, default: None
		    Single stock symbol (ticker), array-like object of symbols or
		    DataFrame with index containing stock symbols
		start : string, (defaults to '1/1/2010')
		    Starting date, timestamp. Parses many different kind of date
		    representations (e.g., 'JAN-01-2010', '1/1/10', 'Jan, 1, 1980')
		end : string, (defaults to today)
		    Ending date, timestamp. Same format as starting date.
		retry_count : int, default: 3
		    Number of times to retry query request.
		pause : numeric, default: 0.001
		    Time, in seconds, to pause between consecutive queries of chunks. If
		    single value given for symbol, represents the pause between retries.
		adjust_price : bool, default: False
		    If True, adjusts all prices in hist_data ('Open', 'High', 'Low',
		    'Close') based on 'Adj Close' price. Adds 'Adj_Ratio' column and drops
		    'Adj Close'.
		ret_index : bool, default: False
		    If True, includes a simple return index 'Ret_Index' in hist_data.
		chunksize : int, default: 25
		    Number of symbols to download consecutively before intiating pause.
		interval : string, default: 'd'
		    Time interval code, valid values are 'd' for daily, 'w' for weekly,
		    'm' for monthly and 'v' for dividend.
		
		Returns
		-------
		hist_data : DataFrame (str) or Panel (array-like object, DataFrame)
	**/
	static public function get_data_yahoo(?symbols:Dynamic, ?start:Dynamic, ?end:Dynamic, ?retry_count:Dynamic, ?pause:Dynamic, ?adjust_price:Dynamic, ?ret_index:Dynamic, ?chunksize:Dynamic, ?interval:Dynamic):Dynamic;
	static public function get_quote_google(symbols:Dynamic):Dynamic;
	/**
		Get current yahoo quote
		
		Returns a DataFrame
	**/
	static public function get_quote_yahoo(symbols:Dynamic):Dynamic;
	static public function is_list_like(arg:Dynamic):Dynamic;
	static public function lmap(args:Dynamic, kwargs:Dynamic):Dynamic;
	/**
		Read CSV (comma-separated) file into DataFrame
		
		Also supports optionally iterating or breaking of the file
		into chunks.
		
		Parameters
		----------
		filepath_or_buffer : string or file handle / StringIO
		    The string could be a URL. Valid URL schemes include
		    http, ftp, s3, and file. For file URLs, a
		    host is expected. For instance, a local file could be
		    file ://localhost/path/to/table.csv
		sep : string, default ','
		    Delimiter to use. If sep is None, will try to automatically determine
		    this. Regular expressions are accepted.
		engine : {'c', 'python'}
		    Parser engine to use. The C engine is faster while the python engine is
		    currently more feature-complete.
		lineterminator : string (length 1), default None
		    Character to break file into lines. Only valid with C parser
		quotechar : string (length 1)
		    The character used to denote the start and end of a quoted item. Quoted
		    items can include the delimiter and it will be ignored.
		quoting : int or csv.QUOTE_* instance, default None
		    Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of
		    QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).
		    Default (None) results in QUOTE_MINIMAL behavior.
		skipinitialspace : boolean, default False
		    Skip spaces after delimiter
		escapechar : string (length 1), default None
		    One-character string used to escape delimiter when quoting is QUOTE_NONE.
		dtype : Type name or dict of column -> type, default None
		    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}
		    (Unsupported with engine='python')
		compression : {'gzip', 'bz2', 'infer', None}, default 'infer'
		    For on-the-fly decompression of on-disk data. If 'infer', then use gzip or
		    bz2 if filepath_or_buffer is a string ending in '.gz' or '.bz2',
		    respectively, and no decompression otherwise. Set to None for no
		    decompression.
		dialect : string or csv.Dialect instance, default None
		    If None defaults to Excel dialect. Ignored if sep longer than 1 char
		    See csv.Dialect documentation for more details
		header : int, list of ints, default 'infer'
		    Row number(s) to use as the column names, and the start of the
		    data.  Defaults to 0 if no ``names`` passed, otherwise ``None``. Explicitly
		    pass ``header=0`` to be able to replace existing names. The header can be
		    a list of integers that specify row locations for a multi-index on the
		    columns E.g. [0,1,3]. Intervening rows that are not specified will be
		    skipped (e.g. 2 in this example are skipped). Note that this parameter
		    ignores commented lines and empty lines if ``skip_blank_lines=True``, so header=0
		    denotes the first line of data rather than the first line of the file.
		skiprows : list-like or integer, default None
		    Line numbers to skip (0-indexed) or number of lines to skip (int)
		    at the start of the file
		index_col : int or sequence or False, default None
		    Column to use as the row labels of the DataFrame. If a sequence is given, a
		    MultiIndex is used. If you have a malformed file with delimiters at the end
		    of each line, you might consider index_col=False to force pandas to _not_
		    use the first column as the index (row names)
		names : array-like, default None
		    List of column names to use. If file contains no header row, then you
		    should explicitly pass header=None
		prefix : string, default None
		    Prefix to add to column numbers when no header, e.g 'X' for X0, X1, ...
		na_values : str, list-like or dict, default None
		    Additional strings to recognize as NA/NaN. If dict passed, specific
		    per-column NA values
		true_values : list, default None
		    Values to consider as True
		false_values : list, default None
		    Values to consider as False
		keep_default_na : bool, default True
		    If na_values are specified and keep_default_na is False the default NaN
		    values are overridden, otherwise they're appended to
		parse_dates : boolean, list of ints or names, list of lists, or dict, default False
		    If True -> try parsing the index.
		    If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.
		    If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column.
		    {'foo' : [1, 3]} -> parse columns 1, 3 as date and call result 'foo'
		    A fast-path exists for iso8601-formatted dates.
		keep_date_col : boolean, default False
		    If True and parse_dates specifies combining multiple columns then
		    keep the original columns.
		date_parser : function, default None
		    Function to use for converting a sequence of string columns to an
		    array of datetime instances. The default uses dateutil.parser.parser
		    to do the conversion. Pandas will try to call date_parser in three different
		    ways, advancing to the next if an exception occurs: 1) Pass one or more arrays
		    (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string
		    values from the columns defined by parse_dates into a single array and pass
		    that; and 3) call date_parser once for each row using one or more strings
		    (corresponding to the columns defined by parse_dates) as arguments.
		dayfirst : boolean, default False
		    DD/MM format dates, international and European format
		thousands : str, default None
		    Thousands separator
		comment : str, default None
		    Indicates remainder of line should not be parsed. If found at the
		    beginning of a line, the line will be ignored altogether. This parameter
		    must be a single character. Like empty lines (as long as ``skip_blank_lines=True``),
		    fully commented lines are ignored by the parameter `header`
		    but not by `skiprows`. For example, if comment='#', parsing
		    '#empty\na,b,c\n1,2,3' with `header=0` will result in 'a,b,c' being
		    treated as the header.
		decimal : str, default '.'
		    Character to recognize as decimal point. E.g. use ',' for European data
		nrows : int, default None
		    Number of rows of file to read. Useful for reading pieces of large files
		iterator : boolean, default False
		    Return TextFileReader object
		chunksize : int, default None
		    Return TextFileReader object for iteration
		skipfooter : int, default 0
		    Number of lines at bottom of file to skip (Unsupported with engine='c')
		converters : dict, default None
		    Dict of functions for converting values in certain columns. Keys can either
		    be integers or column labels
		verbose : boolean, default False
		    Indicate number of NA values placed in non-numeric columns
		delimiter : string, default None
		    Alternative argument name for sep. Regular expressions are accepted.
		encoding : string, default None
		    Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python
		    standard encodings
		    <https://docs.python.org/3/library/codecs.html#standard-encodings>`_
		squeeze : boolean, default False
		    If the parsed data only contains one column then return a Series
		na_filter : boolean, default True
		    Detect missing value markers (empty strings and the value of na_values). In
		    data without any NAs, passing na_filter=False can improve the performance
		    of reading a large file
		usecols : array-like, default None
		    Return a subset of the columns.
		    Results in much faster parsing time and lower memory usage.
		mangle_dupe_cols : boolean, default True
		    Duplicate columns will be specified as 'X.0'...'X.N', rather than 'X'...'X'
		tupleize_cols : boolean, default False
		    Leave a list of tuples on columns as is (default is to convert to
		    a Multi Index on the columns)
		error_bad_lines : boolean, default True
		    Lines with too many fields (e.g. a csv line with too many commas) will by
		    default cause an exception to be raised, and no DataFrame will be returned.
		    If False, then these "bad lines" will dropped from the DataFrame that is
		    returned. (Only valid with C parser)
		warn_bad_lines : boolean, default True
		    If error_bad_lines is False, and warn_bad_lines is True, a warning for each
		    "bad line" will be output. (Only valid with C parser).
		infer_datetime_format : boolean, default False
		    If True and parse_dates is enabled for a column, attempt to infer
		    the datetime format to speed up the processing
		skip_blank_lines : boolean, default True
		    If True, skip over blank lines rather than interpreting as NaN values
		
		Returns
		-------
		result : DataFrame or TextParser
	**/
	static public function read_csv(filepath_or_buffer:Dynamic, ?sep:Dynamic, ?dialect:Dynamic, ?compression:Dynamic, ?doublequote:Dynamic, ?escapechar:Dynamic, ?quotechar:Dynamic, ?quoting:Dynamic, ?skipinitialspace:Dynamic, ?lineterminator:Dynamic, ?header:Dynamic, ?index_col:Dynamic, ?names:Dynamic, ?prefix:Dynamic, ?skiprows:Dynamic, ?skipfooter:Dynamic, ?skip_footer:Dynamic, ?na_values:Dynamic, ?true_values:Dynamic, ?false_values:Dynamic, ?delimiter:Dynamic, ?converters:Dynamic, ?dtype:Dynamic, ?usecols:Dynamic, ?engine:Dynamic, ?delim_whitespace:Dynamic, ?as_recarray:Dynamic, ?na_filter:Dynamic, ?compact_ints:Dynamic, ?use_unsigned:Dynamic, ?low_memory:Dynamic, ?buffer_lines:Dynamic, ?warn_bad_lines:Dynamic, ?error_bad_lines:Dynamic, ?keep_default_na:Dynamic, ?thousands:Dynamic, ?comment:Dynamic, ?decimal:Dynamic, ?parse_dates:Dynamic, ?keep_date_col:Dynamic, ?dayfirst:Dynamic, ?date_parser:Dynamic, ?memory_map:Dynamic, ?float_precision:Dynamic, ?nrows:Dynamic, ?iterator:Dynamic, ?chunksize:Dynamic, ?verbose:Dynamic, ?encoding:Dynamic, ?squeeze:Dynamic, ?mangle_dupe_cols:Dynamic, ?tupleize_cols:Dynamic, ?infer_datetime_format:Dynamic, ?skip_blank_lines:Dynamic):Dynamic;
	/**
		Read HTML tables into a ``list`` of ``DataFrame`` objects.
		
		Parameters
		----------
		io : str or file-like
		    A URL, a file-like object, or a raw string containing HTML. Note that
		    lxml only accepts the http, ftp and file url protocols. If you have a
		    URL that starts with ``'https'`` you might try removing the ``'s'``.
		
		match : str or compiled regular expression, optional
		    The set of tables containing text matching this regex or string will be
		    returned. Unless the HTML is extremely simple you will probably need to
		    pass a non-empty string here. Defaults to '.+' (match any non-empty
		    string). The default value will return all tables contained on a page.
		    This value is converted to a regular expression so that there is
		    consistent behavior between Beautiful Soup and lxml.
		
		flavor : str or None, container of strings
		    The parsing engine to use. 'bs4' and 'html5lib' are synonymous with
		    each other, they are both there for backwards compatibility. The
		    default of ``None`` tries to use ``lxml`` to parse and if that fails it
		    falls back on ``bs4`` + ``html5lib``.
		
		header : int or list-like or None, optional
		    The row (or list of rows for a :class:`~pandas.MultiIndex`) to use to
		    make the columns headers.
		
		index_col : int or list-like or None, optional
		    The column (or list of columns) to use to create the index.
		
		skiprows : int or list-like or slice or None, optional
		    0-based. Number of rows to skip after parsing the column integer. If a
		    sequence of integers or a slice is given, will skip the rows indexed by
		    that sequence.  Note that a single element sequence means 'skip the nth
		    row' whereas an integer means 'skip n rows'.
		
		attrs : dict or None, optional
		    This is a dictionary of attributes that you can pass to use to identify
		    the table in the HTML. These are not checked for validity before being
		    passed to lxml or Beautiful Soup. However, these attributes must be
		    valid HTML table attributes to work correctly. For example, ::
		
		        attrs = {'id': 'table'}
		
		    is a valid attribute dictionary because the 'id' HTML tag attribute is
		    a valid HTML attribute for *any* HTML tag as per `this document
		    <http://www.w3.org/TR/html-markup/global-attributes.html>`__. ::
		
		        attrs = {'asdf': 'table'}
		
		    is *not* a valid attribute dictionary because 'asdf' is not a valid
		    HTML attribute even if it is a valid XML attribute.  Valid HTML 4.01
		    table attributes can be found `here
		    <http://www.w3.org/TR/REC-html40/struct/tables.html#h-11.2>`__. A
		    working draft of the HTML 5 spec can be found `here
		    <http://www.w3.org/TR/html-markup/table.html>`__. It contains the
		    latest information on table attributes for the modern web.
		
		parse_dates : bool, optional
		    See :func:`~pandas.read_csv` for more details.
		
		tupleize_cols : bool, optional
		    If ``False`` try to parse multiple header rows into a
		    :class:`~pandas.MultiIndex`, otherwise return raw tuples. Defaults to
		    ``False``.
		
		thousands : str, optional
		    Separator to use to parse thousands. Defaults to ``','``.
		
		encoding : str or None, optional
		    The encoding used to decode the web page. Defaults to ``None``.``None``
		    preserves the previous encoding behavior, which depends on the
		    underlying parser library (e.g., the parser library will try to use
		    the encoding provided by the document).
		
		Returns
		-------
		dfs : list of DataFrames
		
		Notes
		-----
		Before using this function you should read the :ref:`gotchas about the
		HTML parsing libraries <html-gotchas>`.
		
		Expect to do some cleanup after you call this function. For example, you
		might need to manually assign column names if the column names are
		converted to NaN when you pass the `header=0` argument. We try to assume as
		little as possible about the structure of the table and push the
		idiosyncrasies of the HTML contained in the table to the user.
		
		This function searches for ``<table>`` elements and only for ``<tr>``
		and ``<th>`` rows and ``<td>`` elements within each ``<tr>`` or ``<th>``
		element in the table. ``<td>`` stands for "table data".
		
		Similar to :func:`~pandas.read_csv` the `header` argument is applied
		**after** `skiprows` is applied.
		
		This function will *always* return a list of :class:`DataFrame` *or*
		it will fail, e.g., it will *not* return an empty list.
		
		Examples
		--------
		See the :ref:`read_html documentation in the IO section of the docs
		<io.read_html>` for some examples of reading in HTML tables.
		
		See Also
		--------
		pandas.read_csv
	**/
	static public function read_html(io:Dynamic, ?match:Dynamic, ?flavor:Dynamic, ?header:Dynamic, ?index_col:Dynamic, ?skiprows:Dynamic, ?attrs:Dynamic, ?parse_dates:Dynamic, ?tupleize_cols:Dynamic, ?thousands:Dynamic, ?encoding:Dynamic):Dynamic;
	/**
		Convert argument to datetime.
		
		Parameters
		----------
		arg : string, datetime, array of strings (with possible NAs)
		errors : {'ignore', 'raise', 'coerce'}, default 'raise'
		    - If 'raise', then invalid parsing will raise an exception
		    - If 'coerce', then invalid parsing will be set as NaT
		    - If 'ignore', then invalid parsing will return the input
		dayfirst : boolean, default False
		    Specify a date parse order if `arg` is str or its list-likes.
		    If True, parses dates with the day first, eg 10/11/12 is parsed as 2012-11-10.
		    Warning: dayfirst=True is not strict, but will prefer to parse
		    with day first (this is a known bug, based on dateutil behavior).
		yearfirst : boolean, default False
		    Specify a date parse order if `arg` is str or its list-likes.
		    - If True parses dates with the year first, eg 10/11/12 is parsed as 2010-11-12.
		    - If both dayfirst and yearfirst are True, yearfirst is preceded (same as dateutil).
		    Warning: yearfirst=True is not strict, but will prefer to parse
		    with year first (this is a known bug, based on dateutil beahavior).
		
		    .. versionadded: 0.16.1
		
		utc : boolean, default None
		    Return UTC DatetimeIndex if True (converting any tz-aware
		    datetime.datetime objects as well).
		box : boolean, default True
		    - If True returns a DatetimeIndex
		    - If False returns ndarray of values.
		format : string, default None
		    strftime to parse time, eg "%d/%m/%Y", note that "%f" will parse
		    all the way up to nanoseconds.
		exact : boolean, True by default
		    - If True, require an exact format match.
		    - If False, allow the format to match anywhere in the target string.
		unit : unit of the arg (D,s,ms,us,ns) denote the unit in epoch
		    (e.g. a unix timestamp), which is an integer/float number.
		infer_datetime_format : boolean, default False
		    If no `format` is given, try to infer the format based on the first
		    datetime string. Provides a large speed-up in many cases.
		
		Returns
		-------
		ret : datetime if parsing succeeded.
		    Return type depends on input:
		
		    - list-like: DatetimeIndex
		    - Series: Series of datetime64 dtype
		    - scalar: Timestamp
		
		    In case when it is not possible to return designated types (e.g. when
		    any element of input is before Timestamp.min or after Timestamp.max)
		    return will have datetime.datetime type (or correspoding array/Series).
		
		Examples
		--------
		Take separate series and convert to datetime
		
		>>> import pandas as pd
		>>> i = pd.date_range('20000101',periods=100)
		>>> df = pd.DataFrame(dict(year = i.year, month = i.month, day = i.day))
		>>> pd.to_datetime(df.year*10000 + df.month*100 + df.day, format='%Y%m%d')
		0    2000-01-01
		1    2000-01-02
		...
		98   2000-04-08
		99   2000-04-09
		Length: 100, dtype: datetime64[ns]
		
		Or from strings
		
		>>> df = df.astype(str)
		>>> pd.to_datetime(df.day + df.month + df.year, format="%d%m%Y")
		0    2000-01-01
		1    2000-01-02
		...
		98   2000-04-08
		99   2000-04-09
		Length: 100, dtype: datetime64[ns]
		
		Date that does not meet timestamp limitations:
		
		>>> pd.to_datetime('13000101', format='%Y%m%d')
		datetime.datetime(1300, 1, 1, 0, 0)
		>>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce')
		NaT
	**/
	static public function to_datetime(arg:Dynamic, ?errors:Dynamic, ?dayfirst:Dynamic, ?yearfirst:Dynamic, ?utc:Dynamic, ?box:Dynamic, ?format:Dynamic, ?exact:Dynamic, ?coerce:Dynamic, ?unit:Dynamic, ?infer_datetime_format:Dynamic):Dynamic;
	/**
		Encode a dict or sequence of two-element tuples into a URL query string.
		
		If any values in the query arg are sequences and doseq is true, each
		sequence element is converted to a separate parameter.
		
		If the query arg is a sequence of two-element tuples, the order of the
		parameters in the output will match the order of parameters in the
		input.
		
		The components of a query arg may each be either a string or a bytes type.
		
		The safe, encoding, and errors parameters are passed down to the function
		specified by quote_via (encoding and errors only if a component is a str).
	**/
	static public function urlencode(query:Dynamic, ?doseq:Dynamic, ?safe:Dynamic, ?encoding:Dynamic, ?errors:Dynamic, ?quote_via:Dynamic):Dynamic;
	static public function urlopen(url:Dynamic, ?data:Dynamic, ?timeout:Dynamic, ?cafile:Dynamic, ?capath:Dynamic, ?cadefault:Dynamic, ?context:Dynamic):Dynamic;
}